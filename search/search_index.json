{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CloudVVUQ documentation","text":"<p>What: CloudVVUQ - python library which allows running simulations in parallel and in the cloud with EasyVVUQ functionality.    </p> <p>Why: To speed up VVUQ and sensitivity analysis (SA) computations using serverless compute services and extend EasyVVUQ library.   </p> <p>How: VVUQ and SA processes require many independent executions of analyzed model. This step is an embarrassingly-parallel problem and can be parallelized using serverless computing.</p> <p>For who: Scientists with some technical background already running VVUQ and SA processes on HPC or local machines. </p> <p>Performance: Using this library you can speed your calculations hundreds or thousands of times.</p> <p>How to use: 1. Have access to one or more cloud providers and choose suitable service for your model. 2. Prepare your model for cloud (see model_preparation section). 3. Install CloudVVUQ locally. 4. Prepare script that starts CloudVVUQ executor / Adapt your existing EasyVVUQ script. 5. Deploy the model (see model_deployment section). Test it on smaller scale. 6. Launch full-scale simulations.   </p>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>This work is supported by the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement Sano No 857533 and carried out within the International Research Agendas programme of the Foundation for Polish Science, co-financed by the European Union under the European Regional Development Fund. This work is also partly supported by the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement ISW No 101016503.</p>"},{"location":"#license","title":"License","text":"<p>The library is published under the MIT license. However, the examples directory is published under the LGPLv3 license.</p>"},{"location":"credentials_configuration/","title":"Credentials configuration","text":""},{"location":"credentials_configuration/#credentials","title":"Credentials","text":"<p>Getting credentials to authorize CloudVVUQ invocation is the first step. Depending on the cloud provider you use there are many ways to do this. Currently, CloudVVUQ supports authorization with two cloud providers:</p> <ul> <li>AWS </li> <li>GCP</li> </ul> <p>It is possible to extend this list by implementing authorization method for your provider in authorizer.py.</p>"},{"location":"credentials_configuration/#aws-credentials","title":"AWS credentials","text":"<p>Full list of possible credentials configurations and documentation is here The easiest and most CloudVVUQ-compatible method would be using: - shared credentials file ( ~/.aws/credentials ) - environment variables</p>"},{"location":"credentials_configuration/#gcp-credentials","title":"GCP credentials","text":"<ul> <li>Create a service account with appropriate permissions (e.g. cloudfunctions.functions.invoke)</li> <li>Create a key for this account -&gt; download credentials file</li> <li>Set a environmental variable in script with path to credentials file:   <pre><code>os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'credentials/gcp_creds.json'\n</code></pre></li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#new-conda-environment-and-use-cloudvvuq-dir","title":"New conda environment and use CloudVVUQ dir","text":"<ol> <li>Clone CloudVVUQ repository</li> <li>Create environment: <pre><code>conda create --name CloudVVUQ  </code></pre> <pre><code>conda activate CloudVVUQ\n</code></pre></li> <li>Install required packages: <pre><code>conda install --file requirements.txt  </code></pre></li> </ol>"},{"location":"installation/#use-existing-environment","title":"Use existing environment","text":"<ol> <li>Clone CloudVVUQ repository</li> <li>Activate your environment</li> <li>Get path to CloudVVUQ wheel file, replace it in the command below.     <pre><code>pip install \"path/to/cloudvvuq/dist/cloudvvuq-*.whl\"\n</code></pre></li> <li>Run it to install CloudVVUQ module</li> </ol>"},{"location":"local_configuration/","title":"Local configuration","text":""},{"location":"local_configuration/#general-configuration-executor","title":"General configuration (Executor)","text":"<ul> <li>Set max load value in executor.run method to a smaller value than configured max concurrency. <pre><code>executor.run(samples, max_load=256)  \n</code></pre></li> <li>Specify your cloud provider to authorize your simulations using provided credentials. <pre><code>executor.run(samples, max_load=256, cloud_provider=\"aws\")\n</code></pre></li> <li>Max TCP value in CloudConnector class (e.g. when require &gt;1000 concurrent simulations). Default 1000. (high level config - work in progress) <pre><code>aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit=1000),  \n</code></pre></li> <li>Timeout value for simulations in CloudConnector class. Default 1 hour. (high level config - work in progress) <pre><code>timeout=aiohttp.ClientTimeout(total=3600)) as session:  \n</code></pre></li> </ul>"},{"location":"local_configuration/#executor-configuration","title":"Executor configuration","text":"<p>In order to run simulations using CloudVVUQ you need to: 1. Provide list of samples.     <pre><code>samples = [{\"x\": 1, \"y\": 2}, {\"x\": 2, \"y\": 3} ...]\n</code></pre> 2. Create Executor and specify endpoint:     <pre><code>url = \"http://localhost:8080/2015-03-31/functions/function/invocations\"\nexecutor = Executor(url)\n</code></pre> 3. Run simulations and wait for it to finish     <pre><code>outputs = executor.run(samples, max_load=256, cloud_provider=\"aws\")\n</code></pre></p>"},{"location":"local_configuration/#easyexecutor-configuration","title":"EasyExecutor configuration","text":"<p>EasyExecutor is responsible for providing EasyVVUQ-related methods and extends Executor general functionality.</p> <p>In order to run EasyVVUQ script as CloudVVUQ you need to: 1. Define sampler and its variances (as you would with EasyVVUQ)     <pre><code>sampler = uq.sampling.SCSampler(vary=vary, polynomial_order=3)\n</code></pre> 2. Create EasyExecutor and attach sampler:     <pre><code>executor = EasyExecutor(url)\nexecutor.set_sampler(sampler, params)\n</code></pre> 3. Draw samples     <pre><code>executor.set_sampler(sampler, params)\n</code></pre> 4. Run simulations and wait for it to finish     <pre><code>outputs = executor.run(samples, max_load=256, cloud_provider=\"aws\")\n</code></pre> 5. Create EasyVVUQ-native campaign object     <pre><code>campaign = executor.create_campaign(\"campaign_name\", \ninput_columns=['F', 'L', 'a', 'D', 'd', 'E'],\noutput_columns=['g1', 'g2', 'g3'])\n</code></pre> From now on you can work on EasyVVUQ object (apply analysis etc.)</p>"},{"location":"model_deployment/","title":"Model deployment","text":""},{"location":"model_deployment/#model-deployment","title":"Model deployment","text":""},{"location":"model_deployment/#aws-lambda","title":"AWS Lambda","text":"<ol> <li>Go to Lambda service.</li> <li>Choose AWS region the closest to you.</li> <li>Click \"Create function\".</li> <li>Choose a deployment option and name the function.<ol> <li>if you choose author-from-scratch then specify your runtime language and use defaults.</li> <li>if you choose container image then select you container image from Elastic Container Registry (see \"Docker image deployment\" how to upload docker image).</li> </ol> </li> <li>Click \"Create function\" and wait for it to be created.</li> <li>If you decided to author-from-scratch then see next sections to see how you can deploy your code.</li> <li>Proceed to function configuration.</li> <li>In general configuration set timeout longer than your model execution maximum time.</li> <li>Allocate enough memory for your function.</li> <li>Go to Function URL and enable it.</li> <li>Model is deployed.</li> </ol>"},{"location":"model_deployment/#copy-paste-deployment","title":"Copy-paste deployment","text":"<p>Probably easiest and most intuitive deployment mode. Just create a hello-world function using web console then replace the code using built-in code editor. Not the best option for large code bases and many dependencies.</p> <p>Create the following structure in the code-editor: <pre><code>model_name\n\u2502   model.py\n\u2502   wrapper.py    \n</code></pre></p> <p>You can make use of folders and add additional files as long as you can correctly call the main model's method from the wrapper. </p>"},{"location":"model_deployment/#zip-file-deployment","title":"Zip file deployment","text":"<p>Documentation</p> <p>Create the following structure and zip files inside: <pre><code>lambda\n\u2502   model.py\n\u2502   wrapper.py    \n</code></pre> Then you can upload the zip file into the lambda functions using web console. If your archive is bigger than 50 MB you must first upload the archive to S3 and then when deploying lambda you specify S3 url to archive.</p> <p>There are alternative deployment modes. Check it out especially if you need dependencies not provided by layers. Guide </p>"},{"location":"model_deployment/#docker-image-deployment","title":"Docker image deployment","text":"<p>Is this section we expect that you have ready-to-deploy container image. If not check the \"Docker image creation and deployment \" section first.</p> <p>First you have to upload built image to AWS Elastic Container Registry: Guide 1. Authenticate the Docker CLI to your Amazon ECR registry 2. Tag your image <code>docker tag SOURCE_IMAGE REPOSITORY_NAME/IMAGE_NAME</code> 3. Push image docker <code>push REPOSITORY_NAME/IMAGE_NAME</code> </p>"},{"location":"model_deployment/#google-cloud-functions","title":"Google Cloud Functions","text":"<ol> <li>Go to Cloud Functions service.</li> <li>Fill the configuration:</li> <li>Name the function</li> <li>Choose GCP region the closest to you.</li> <li>Verify that HTTPS trigger with authentication is selected as a trigger.</li> <li>Expand \"Runtime, build, connections and security setting section\"<ol> <li>Allocate enough memory for your function.</li> <li>Set the timeout longer than your model execution maximum time</li> <li>Set upper limit for maximum number of instances (1000 is fine).</li> <li>Choose the runtime service account. Have one created that allows only GC Function invocations (remember about the principle of the least privilege). Documentation</li> </ol> </li> <li>Click next</li> <li>Choose runtime language, entry point, insert your code by either pasting or uploading a zip archive. </li> <li>Click deploy and wait for it to be ready.</li> <li>Model is deployed.</li> </ol>"},{"location":"model_deployment/#copy-paste-deployment_1","title":"Copy-paste deployment","text":"<p>Probably easiest and most intuitive deployment mode. Just modify a hello-world function using web console then replace the code using built-in code editor. Not the best option for large code bases and many dependencies.</p> <p>Create the following structure in the code-editor: <pre><code>model_name\n\u2502   model.py\n|   requirements.txt\n\u2502   wrapper.py  (aka main.py)    \n</code></pre> Insert your dependencies in requirements.txt file. They will be installed during the deployment. You can make use of folders and add additional files as long as you can correctly call the main model's method from the wrapper. </p>"},{"location":"model_deployment/#zip-archive-deployment","title":"Zip archive deployment","text":"<p>Create the following structure and zip files inside: <pre><code>function\n\u2502   model.py\n|   requirements.txt\n\u2502   wrapper.py  (aka main.py)     \n</code></pre></p> <p>Then you can upload the zip file into the GC Function using web console.</p>"},{"location":"model_deployment/#google-cloud-run","title":"Google Cloud Run","text":""},{"location":"model_deployment/#docker-image-creation-and-deployment","title":"Docker image creation and deployment","text":"<p>Documentation</p> <ol> <li>Have docker installed locally - Download and installation guide</li> <li>Create a Dockerfile in your project (make use of boilerplate code provided below). Example project structure: <pre><code>project\n|   Dockerfile\n\u2502   requirements.txt\n\u2502   wrapper.py\n|   model.py    \n</code></pre></li> <li>Build your image, use cli in project's directory <pre><code>docker build .\n</code></pre></li> <li> <p>(Optional but recommended) Test image locally <pre><code>docker run -p 8080:8080 &lt;image_id&gt;\n</code></pre> Replace image_id. You can list all image ids using <pre><code>docker images\n</code></pre> If your image is running as a container you can paste  <pre><code>url = \"http://127.0.0.1:8080\"\n</code></pre> into you local (client) code where you have CloudVVUQ installed and try to compute a single sample. <pre><code>outputs = executor.run(samples[1], max_load=1)\n</code></pre></p> </li> <li> <p>Possible misconfigurations and remedies:  </p> <ul> <li>request does not reach the container<ul> <li>check if port 8080 is opened  </li> </ul> </li> <li>container crashed due to misconfigured paths  <ul> <li>check if you correctly imported your model in the wrapper code  </li> <li>check if all dependencies are installed</li> </ul> </li> </ul> </li> </ol>"},{"location":"model_deployment/#docker-image-templates","title":"Docker image templates","text":""},{"location":"model_deployment/#aws-lambda-using-aws-base-image","title":"AWS Lambda using AWS base-image","text":"<pre><code>FROM public.ecr.aws/lambda/python:3.9\nCOPY requirements.txt  .\nRUN  pip3 install -r requirements.txt --target \"${LAMBDA_TASK_ROOT}\"\nCOPY model.py ${LAMBDA_TASK_ROOT}\nCMD [ \"app.handler\" ]\n</code></pre>"},{"location":"model_deployment/#google-cloud-run_1","title":"Google Cloud Run","text":"<pre><code>FROM python:3.10-slim\nADD app/requirements.txt .\nRUN pip3 install -r requirements.txt\n\nCOPY /app /app\nWORKDIR /app\nEXPOSE 8080:8080\nCMD [\"python3\", \"main.py\"]\n</code></pre>"},{"location":"model_preparation/","title":"Model preparation","text":""},{"location":"model_preparation/#choosing-deployment-type","title":"Choosing deployment type","text":"<p>In order to successfully deploy your model for CloudVVUQ usage you must consider few aspects:</p> <ul> <li>Available services to you</li> <li>Source code language<ul> <li>Services such as Google Cloud Functions do not support every language as runtime. If your code is written in e.g. Octave you have to use services that support running custom container images.  </li> </ul> </li> <li>Maximum execution time of your model<ul> <li>If you deploy a model which takes 30 min to compute you should not use AWS Lambda but Google Cloud Run instead. More about the service-specific quotas in cloud configuration section.</li> </ul> </li> <li>Dependencies<ul> <li>If you need to package custom software or dependencies along the model's source code container image is your best option as deployment type.</li> </ul> </li> </ul>"},{"location":"model_preparation/#model-preparation","title":"Model preparation","text":"<p>In this step we assume you have a working computational model.  </p> <p>To prepare the model for deployment you must:  </p> <ul> <li>specify requirements and dependencies  </li> <li>support execution of the model which takes path to a json-file with model arguments/samples and saves results to json-file  </li> </ul> <p>For the 2nd step provide a command-line interface for your model such as: <pre><code>python3 model.py input_file.json\n</code></pre>   Your model.py file must take one argument - input_file which is a path to file with your sample's values packed in json.</p> <p>Below is copy-ready code (in Python).   <pre><code>import sys\nimport json\njson_input = sys.argv[1]\nwith open(json_input, 'r') as fd:\ninputs = json.load(fd)\n</code></pre>   Then you have to unpack the values in before you pass them into the model e.g.:   <pre><code>F = inputs['F']\nL = inputs['L']\na = inputs['a']\nD = inputs['D']\n</code></pre>   Then you can pass the variables into your model execution function.   <pre><code>g1, g2, g3 = solve_sample(F, L, a, D)\n</code></pre>   After the model has been solved you return the results. You can do this like this:   <pre><code>with open(inputs['outfile'], 'w') as fd:\njson.dump({'g1': g1, 'g2': g2, 'g3': g3}, fd)\n</code></pre></p> <p>This is simple communication by file. The parent-process passes the filepath to child-process. Child process reads the sample's values from the file, executes the model with those values and returns the results to the parent in another file. The parent then sends back the response with model outputs to the client.  </p> <p>Alternatively if your model is written in Python you can import the interface method in the \"server\" code   <pre><code>output = model_solve_sample(inputs)\n</code></pre>   This way you can skip the communication by files. Also, you slightly reduce the execution time because you don't start and new process but reuse existing one. Noticeable difference only for very fast models (&lt;1s).</p>"},{"location":"model_preparation/#wrapper-code-server-code","title":"Wrapper code (\"server\" code)","text":"<p>Each service requires slightly different wrapper code. Here are boilerplate code for each service. Reuse existing code are replace the 'model.py' with relative path to your model's code.</p>"},{"location":"model_preparation/#aws-lambda-layers","title":"AWS Lambda - layers","text":"<pre><code>import json\nimport subprocess\ndef lambda_handler(event, context):\ninput_json = event['body']  # extract sample\ninput_file = f\"/tmp/input_{input_json['input_id']}.json\"  \ninput_json[\"outfile\"] = f\"/tmp/output_{input_json['input_id']}.json\"  # add outfile path to sample dict\nwith open(input_file, \"w+\") as f:  # save sample to file\njson.dump(input_json, f)\nsubprocess.run([\"python3\", \"model.py\", input_file])  # start a new process with model computation and pass sample \nwith open(input_json[\"outfile\"]) as f:  # read results from output file\nresult = json.load(f)\nresult.update(input_json) \nreturn result  # return model outputs to the client\n</code></pre>"},{"location":"model_preparation/#aws-lambda-image","title":"AWS Lambda - image","text":"<pre><code>import json\nimport subprocess\ndef lambda_handler(event, context):\ninput_json = json.loads(event['body'])\ninput_file = f\"/tmp/input_{input_json['input_id']}.json\"\ninput_json[\"outfile\"] = f\"/tmp/output_{input_json['input_id']}.json\"\nwith open(input_file, \"w+\") as f:\njson.dump(input_json, f)\nsubprocess.run([\"python3\", \"model.py\", input_file])\nwith open(input_json[\"outfile\"]) as f:\nresult = json.load(f)\nresult.update(input_json)\nreturn {\n\"statusCode\": 200,\n\"body\": json.dumps(result),\n\"headers\": {\n\"content-type\": \"application/json\"\n},\n\"isBase64Encoded\": False\n}\n</code></pre>"},{"location":"model_preparation/#google-cloud-run","title":"Google Cloud Run","text":"<pre><code>import os\nimport json\nimport subprocess\nfrom flask import Flask, request\napp = Flask(__name__)\n@app.route(\"/\", methods=[\"POST\"])\ndef run_script():\ninput_json = request.get_json()\ninput_file = f\"/tmp/input_{input_json['input_id']}.json\"\ninput_json[\"outfile\"] = f\"/tmp/output_{input_json['input_id']}.json\"\nwith open(input_file, \"w+\") as f:\njson.dump(input_json, f)\nsubprocess.run([\"python3\", \"model.py\", input_file], capture_output=True)\nwith open(input_json[\"outfile\"]) as f:\nresult = json.load(f)\nresult.update(input_json)\nos.remove(input_file)\nos.remove(input_json[\"outfile\"])\nreturn result\nif __name__ == \"__main__\":\napp.run(debug=False, host=\"0.0.0.0\", port=int(os.environ.get(\"PORT\", 8080)))\n</code></pre>"},{"location":"model_preparation/#google-cloud-functions","title":"Google Cloud Functions","text":"<pre><code>import json\nimport subprocess\ndef run_script(request):\ninput_json = request.get_json()\ninput_file = f\"/tmp/input_{input_json['input_id']}.json\"\ninput_json[\"outfile\"] = f\"/tmp/output_{input_json['input_id']}.json\"\nwith open(input_file, \"w+\") as f:\njson.dump(input_json, f)\nsubprocess.run([\"python3\", \"model.py\", input_file])\nwith open(input_json[\"outfile\"]) as f:\nresult = json.load(f)\nresult.update(input_json)\nreturn result\n</code></pre>"},{"location":"trivia/","title":"Trivia","text":"<ul> <li>Your deployments can go into the inactive/idle state after a period (e.g. 1-2 weeks) of inactivity (no traffic). It means that you may have to manually restore it (AWS Lambda) or expect initial degradation of performance (in response status codes and invocation times).  </li> </ul>"},{"location":"cloud_configuration/aws_lambda/","title":"AWS Lambda","text":""},{"location":"cloud_configuration/aws_lambda/#aws-lambda","title":"AWS Lambda","text":"<ul> <li>CPU proportional to memory (1769 MB == 1 vCPU - study)</li> <li>Broad but limited set of available runtimes </li> </ul>"},{"location":"cloud_configuration/aws_lambda/#lambda-specific-configuration","title":"Lambda specific configuration","text":"<ul> <li>Allow function url in configuration</li> </ul>"},{"location":"cloud_configuration/aws_lambda/#lambda-limits-docs","title":"Lambda limits - docs","text":"<ul> <li>15 minutes - max timeout value</li> <li>1000 concurrent instances (possible to request quota increase)</li> </ul>"},{"location":"cloud_configuration/aws_lambda/#lambda-layers-docs","title":"Lambda layers - docs","text":"<ul> <li>Used to add dependencies (e.g. numpy, pandas) to your functions (deployed as a zip file)</li> </ul>"},{"location":"cloud_configuration/aws_lambda/#lambda-container-images-docs","title":"Lambda container images - docs","text":"<ul> <li>Alternative deployment type for Lambda. Easier to package code with its dependencies</li> <li>More customizable option than layers. Allows running code in languages not supported by Lambda (e.g. Matlab, Octave)</li> <li>Requires additional development work to create a Dockerfile and image push to Elastic Container Registry (ECR)</li> </ul>"},{"location":"cloud_configuration/general_configuration/","title":"General configuration","text":""},{"location":"cloud_configuration/general_configuration/#general-configuration","title":"General configuration","text":"<ul> <li>Set function timeout value accordingly:</li> <li>Longest model execution &lt; Function timeout &lt; Max function timeout</li> <li>Choose the closest region for deployment (pricing rarely differs between regions)</li> <li>Before running large-scale simulations, use a pricing calculators to estimate total cost.</li> <li>Monitor resources usage and billing to prevent huge bill.</li> </ul>"},{"location":"cloud_configuration/google_cloud_functions/","title":"Google Cloud Functions","text":""},{"location":"cloud_configuration/google_cloud_functions/#google-cloud-functions-gcf","title":"Google Cloud Functions (GCF)","text":"<ul> <li>CPU proportional to memory (max 8GB - 1st gen)</li> <li>Only zipped/pasted code is possible to deploy</li> <li>Broad but limited set of available runtimes</li> <li>Dependencies are specified requirements.txt file (for Python)</li> </ul>"},{"location":"cloud_configuration/google_cloud_functions/#gcf-limits-docs","title":"GCF Limits - docs","text":"<ul> <li>9min (1st gen) / 60min (2nd gen) - max timeout value</li> </ul>"},{"location":"cloud_configuration/google_cloud_run/","title":"Google Cloud Run","text":""},{"location":"cloud_configuration/google_cloud_run/#google-cloud-run-gcr","title":"Google Cloud Run (GCR)","text":"<ul> <li>Containers not Functions</li> <li>CPU and memory are independent</li> <li>For more resource-demanding models</li> </ul>"},{"location":"cloud_configuration/google_cloud_run/#gcr-limits-docs","title":"GCR limits - docs","text":"<ul> <li>60min - max timeout value</li> </ul>"},{"location":"cloud_configuration/google_cloud_run/#gcr","title":"GCR","text":"<ul> <li>More customizable option than GC Functions. Allows running code in languages not supported by GCF (e.g. Matlab, Octave)</li> <li>Requires additional development work to create a Dockerfile and image push to Google Container Registry</li> </ul>"},{"location":"examples/examples_fusion/","title":"Fusion","text":""},{"location":"examples/examples_fusion/#fusion-example","title":"Fusion example","text":"<p>Adapted for CloudVVUQ from EasyVVUQ tutorials. Original Fusion tutorial here.</p> <p>The examples/fusion directory contains ready-to-deploy source code for deployment types such as:</p> <ul> <li>Google Cloud Run (Python)</li> </ul> <p>Deploy the code to your service and use your new endpoint in the script. Remember to configure executor.run (max load and proper provider).</p>"},{"location":"examples/examples_ishigami/","title":"Ishigami","text":""},{"location":"examples/examples_ishigami/#ishigami-example","title":"Ishigami example","text":"<p>Adapted for CloudVVUQ from EasyVVUQ tutorials. Original Ishigami(SC) tutorial here.</p> <p>The examples/ishigami directory contains ready-to-deploy source code for deployment types such as:</p> <ul> <li>Google Cloud Functions (Python)</li> </ul> <p>Deploy the code to your service and use your new endpoint in the script. Remember to configure executor.run (max load and proper provider).</p>"},{"location":"examples/examples_tube_deflection/","title":"Tube Deflection","text":""},{"location":"examples/examples_tube_deflection/#tube-deflection-example","title":"Tube Deflection example","text":"<p>Adapted for CloudVVUQ from EasyVVUQ tutorials. Original tutorial here.</p> <p>The examples/tube_deflection directory contains ready-to-deploy source code for deployment types such as:</p> <ul> <li>Google Cloud Functions (Python)</li> <li>Google Cloud Run (Python)</li> <li>Google Cloud Run (source code translated to Octave)</li> <li>AWS Lambda (zip with layers)</li> <li>AWS Lambda (image)</li> </ul> <p>All are compatible with tube_deflection_tutorial.py script. Deploy the code to your service and use your new endpoint in the script. Remember to configure executor.run (max load and proper provider).</p>"}]}